{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt notebook with API-first structure\n",
    "In the previous attempt (ocr-test.ipynb), I got all the pieces working, but the code was difficult to read, and made moving to a relation database difficult. I'll prototype an \"API-first notebook\" to see if a that would be a better structure to bridge the data/backend/frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installs, imports\n",
    "%pip install -q \\\n",
    "    pandas\n",
    "\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \"API\" first, where each function can be replaced by a simple API call\n",
    "\n",
    "# \n",
    "# *** IN PSEUDO CODE ***\n",
    "# \n",
    "\n",
    "## \"SERVICES\"\n",
    "## notebook: pure functions\n",
    "## webdev:   eg lambda services, easy to isolate and scale horizontally as needed\n",
    "\n",
    "# upload images\n",
    "def uploadImage(rawFileURL):\n",
    "    # notebook: move files from `upload` folder to `rawFiles` folder\n",
    "    # webdev: write to temporary storage, keep files in case crop doesn't do a good job and needs to be reverted by the user\n",
    "    uploadedFileURL = './store/rawFiles/' + rawFileURL.split('/')[-1]\n",
    "    shutil.move(rawFileURL, uploadedFileURL)\n",
    "    return uploadedFileURL\n",
    "# crop/convert/postprocess images\n",
    "def cropImages(rawFiles):\n",
    "    # write to storage\n",
    "    # write to postProcessedFiles table\n",
    "    return [postProcessedFile1.Id, postProcessedFile2.Id, postProcessedFile3.Id] # could return success/failure, but prefer to return IDs for reference, might want to return objects depending on how it'll be used later\n",
    "# ocr images to create receiptTexts\n",
    "def ocrImage(croppedImage):\n",
    "    # contains data for boundingbox, text\n",
    "    # contains reference to filename\n",
    "    return [receiptText1.Id, receiptText2.Id, receiptText3.Id]\n",
    "# create receipt\n",
    "# def createReceipt(receiptTexts):\n",
    "#     # contains refrence to receiptTexts, filenames via receiptTexts # note one receipt could have multiple images, and texts across these images\n",
    "#     # write to receipts table\n",
    "#     return receipt1.Id\n",
    "\n",
    "## \"DATA\"\n",
    "## notebook: pandas dataframes-->ORM-->to API external call directly\n",
    "## webdev:   eg API create endpoints\n",
    "\n",
    "# define/import referenceItems\n",
    "def writeReferenceItem(name, quantity, unitOfMeasure, price, pricePerWeight, referenceUrl):\n",
    "    # contains data for name, quantity, unitOfMeasure, price, pricePerWeight, referenceUrl\n",
    "    # write to referenceItems table\n",
    "    return referenceItem1.Id\n",
    "\n",
    "# define/import search querries tied to referenceItems\n",
    "def writeEligibleProduct(productName=\"Schar Gluten Free Hot Dog Buns\", referenceItem=\"Hot dog buns\"):\n",
    "    # contains data for productName, referenceItem\n",
    "    # write to eligibleProducts table\n",
    "    return searchQuerry1.Id\n",
    "\n",
    "def writeEligibleExpense(description, amount, date, receiptTextId,referenceItemId):\n",
    "    # contains data for description, amount, date, receiptTextId,referenceItem\n",
    "    # write to eligibleExpenses table\n",
    "    return eligibleExpense1.Id\n",
    "\n",
    "## \"PROCESSING\"\n",
    "## notebook: impure functions on pandas dataframes\n",
    "## webdev:   backend controllers / helpers, harder to isolate\n",
    "\n",
    "# parse receiptTexts against possible product names to find and create eligible eligibleExpenses (description, amount, date, searchQuerry, referenceItem)\n",
    "def checkForEligibleProductName(receiptTextId):\n",
    "    eligibleProduct = None\n",
    "    def checkProductName(receiptTextId):\n",
    "        # check if text in receiptText is in eligibleProducts\n",
    "        return True\n",
    "    def lookForProductPrice(receiptTextId):\n",
    "        # bunch of magic here\n",
    "        return price\n",
    "    if checkProductName(receiptTextId):\n",
    "        price = lookForProductPrice(receiptTextId)\n",
    "        eligibleProduct = ( checkProductName(receiptTextId), lookForProductPrice(receiptTextId) )\n",
    "    return eligibleProduct\n",
    "\n",
    "def parseTextForEligibleExpenses(receiptTextId): \n",
    "    if checkForEligibleProductName(receiptTextId):\n",
    "        writeEligibleExpense(priceEach, quantity, receiptTextId, referenceItemId)\n",
    "    return # success/failure? not sure yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding\n",
    "\n",
    "There's still a step for seeding the referenceItems table. In a notebook, that's loading the data in pandas dataframe; but a product that's data that is already in the app for all users to use, and this is problematic in this data structure because there should be referenceItems that are public to everyone and private to the user. For simplicity, I will assume all referenceItems are shared between all users for the purpose of the prototype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDING\n",
    "# create referenceItems\n",
    "referenceitems = pd.read_csv('referenceitems.csv')\n",
    "for referenceitem in referenceitems:\n",
    "    createReferenceItem(\n",
    "        referenceitem['name'], \n",
    "        referenceitem['quantity'], \n",
    "        referenceitem['unitOfMeasure'], \n",
    "        referenceitem['price'], \n",
    "        referenceitem['pricePerWeight'], \n",
    "        referenceitem['referenceUrl']\n",
    "        # TODO: likely should have one to many relationship with searchQuerries by ID (i.e. different text strings it appears as on receipts)\n",
    "    )\n",
    "\n",
    "# create searchQuerries, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function stuff\n",
    "\n",
    "This is where the two worlds meet, but are different in how they would be approached. \n",
    "\n",
    "A notebooks flow would look like: \n",
    "- for image in images_in_folder process and ocr all images; analyze data\n",
    "- for each string for each receipt: look for eligible expenses; analyze data\n",
    "- for each eligible expense join on reference item info; analyze data\n",
    "- export a single table with all expenses and information\n",
    "\n",
    "(!) this is the main difference in the two approaches / ways of thinking\n",
    "It's a bit easier to think in batches in a notebook (eg ocr all images, parse all texts, etc.) but that's not good for a webdev flow (mostly because it doesn't work well with relational data).\n",
    "\n",
    "consider the user journey:\n",
    "- select images to upload, wait\n",
    "- get a list of receipts to manually review, edit expenses, save (and ideally mark as reviewed, but not for minimum viable prototype)\n",
    "\n",
    "so a webdev flow would look like:\n",
    "1. select one or multiple images, upload\n",
    "2. background chron job to process ocr on each image, look for eligible expenses, match to reference items\n",
    "3. line item level update mutations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one approach is waterfall\n",
    "\n",
    "- main(): \n",
    "    - \"upload\" images (for file in files_in_folder)\n",
    "        - cropImage\n",
    "            - ocrImage (returns receiptTexts)\n",
    "                - parse text for eligible expenses\n",
    "\n",
    "another approach is to do it async as chron jobs, many advantages (scalability, error handling)\n",
    "- \"upload\" images (go through folder)\n",
    "- cropImages that were uploaded but not cropped\n",
    "- ocrImage that were cropped but not OCRed\n",
    "- parse receiptTexts for eligible expenses line items\n",
    "- chron() to run every so often\n",
    "\n",
    "... on the front-end the user will then pick up this data and edit individual Expense line items\n",
    "\n",
    "The async chron job approach above might be a good match for notebooks in the sense that parse each step in bulk before to look at the results before building the block of code. Arguably code can be written sequentially right away, or refactored later, but my objective is to make it easy on both sides to be able to easily build support tooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attempt: waterfall (see `main()` in [ocr-test.ipynb])\n",
    "# I originally tried writing my notebook in the first approach, where I would create a function in a block above the main function block, and have the main function run the `for` loop with reference to all the individual pieces, but I found it hard to read and edit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second attempt: async\n",
    "\n",
    "import time # mostly for testing\n",
    "import threading\n",
    "\n",
    "stop_flag = False\n",
    "\n",
    "def chron():\n",
    "    while not stop_flag:\n",
    "        # check for raw files to crop\n",
    "        # ... probably by looking up at a table of rawFiles that were \"uploaded\"\n",
    "        #     ^ this is the key difference, there isn't an entire dataframe being passed to the next step\n",
    "        # ... tempNewRawFiles = ...\n",
    "        # cropImages(tempNewRawFiles)\n",
    "\n",
    "        # check for croppedImages to ocr\n",
    "        # ... tempNewCroppedImages = ...\n",
    "        # ocrImage(tempNewCroppedImages)\n",
    "\n",
    "        # check for receiptTexts\n",
    "        # ... tempNewReceiptTexts = ...\n",
    "        # parseTextForEligibleExpenses(tempNewReceiptTexts)\n",
    "\n",
    "        # ðŸ now there should be Expenses created, ready for user to manually review and modify\n",
    "\n",
    "        time.sleep(0.500) # slow things down for testing\n",
    "        pass\n",
    "\n",
    "def stop_chron():\n",
    "    global stop_flag\n",
    "    stop_flag = True\n",
    "    watch_thread.join()  # Wait for the thread to finish\n",
    "    pass\n",
    "\n",
    "watch_thread = threading.Thread(target=chron)\n",
    "watch_thread.start()\n",
    "# elsewhere I can use `stop_chron()` to stop the thread\n",
    "\n",
    "# kick things off, note this parrallels nicely what the action the use would take\n",
    "def main():\n",
    "    # \"upload\" images\n",
    "    # rawFiles = ... # from os folder list etc\n",
    "    # for rawFile in rawFiles:\n",
    "    #     uploadImage(rawFile)\n",
    "    pass\n",
    "\n",
    "# from a webdev perspective I would have a docker instance running the above code\n",
    "# here I turn everything off for the purpose of using the notebook\n",
    "# stop_chron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubbish!\n",
    "\n",
    "Threading has little to do with the code structure of a notebook, the issue is not having relational fields and not being able to pass and refer to records (and their information) by their ids. Having chron jobs could mirror having services in a web app, but doesn't help connect the webdev and notebook, and certainly doesn't make writing notebooks any easier. \n",
    "\n",
    "I can have the same structure without threads / services.\n",
    "\n",
    "Also, in my script, pandas was just used to store info in table, which doesn't help much at all. I could just write to/from a db right away, but this goes back to the pandas vs direct db vs orm vs api discussion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third attempt\n",
    "\n",
    "# - \"upload\" images (go through folder) -- i'll take it out for now\n",
    "\n",
    "def cropImages(rawFiles):\n",
    "    # ... do the things\n",
    "    # store ids of processed files\n",
    "    # return file ids\n",
    "    pass\n",
    "\n",
    "def ocrImage(croppedImage):\n",
    "    # ... do the things\n",
    "    # store ids of text strings and image id (needed for UI)\n",
    "    pass\n",
    "\n",
    "def createReceipt(receiptTexts):\n",
    "    # take in receiptTexts\n",
    "    # store id of receipt along with receiptTexts\n",
    "    pass\n",
    "\n",
    "def parseTextForEligibleExpenses(receiptTextId): #one at a time... but by id because need dig around the receipt for price... so maybe by entire receipt?\n",
    "    # ... do the things\n",
    "    #   - check for eligible product names, record referenceItem id\n",
    "    #   - check for prices, record price\n",
    "    #   - create eligible expenses\n",
    "    # store ids of eligible expenses\n",
    "    # return expense ids\n",
    "    pass\n",
    "\n",
    "croppedImageIDs = cropImages(['./store/rawFiles/1.jpg'])\n",
    "receiptTexts = ocrImage(croppedImageIDs) # but for loop\n",
    "eligibleExpenses = parseTextForEligibleExpenses(receiptTexts) # but for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classic notebook approach I could have eg a pandas dataframe for receiptTexts and left join reference ids for matching expenses, then drop non-expense rows, and there's my table of expenses with refrence to receeiptTexts, but then I would still need to parse it in different ways to get separate relational tables, which brings it back to the original problem. \n",
    "\n",
    "I could just have ids on multiple data frames and the .to_sql it and done. It is a bit of a pain to write the ORM schema twice though (I could use AI to convert, but that's not ideal because of mistakes). \n",
    "\n",
    "Beyond that it's having the code structure be based on reference IDs instead of storing values with joins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           boundingBox  \\\n",
      "0    [[np.int32(141), np.int32(101)], [np.int32(619...   \n",
      "1    [[np.int32(87), np.int32(232)], [np.int32(594)...   \n",
      "2    [[np.int32(179), np.int32(270)], [np.int32(499...   \n",
      "3    [[np.int32(103), np.int32(304)], [np.int32(237...   \n",
      "4    [[np.int32(245), np.int32(309)], [np.int32(265...   \n",
      "..                                                 ...   \n",
      "218  [[np.int32(217), np.int32(3863)], [np.int32(27...   \n",
      "219  [[np.int32(308), np.int32(3863)], [np.int32(41...   \n",
      "220  [[np.int32(421), np.int32(3869)], [np.int32(60...   \n",
      "221  [[np.float64(150.34478822279527), np.float64(4...   \n",
      "222  [[np.float64(160.9563638994289), np.float64(12...   \n",
      "\n",
      "                                   text imageFileId  \n",
      "0                              IFRESHCO         123  \n",
      "1    CARLETON PLACE LANSDOWNE FRESHCO _         123  \n",
      "2                  110 Lansdowne Avenue         123  \n",
      "3                             Car Ieton         123  \n",
      "4                                     P         123  \n",
      "..                                  ...         ...  \n",
      "218                                 COD         123  \n",
      "219                             Nuaalcc         123  \n",
      "220                         To Uti Ccoo         123  \n",
      "221                                card         123  \n",
      "222                                 Yog         123  \n",
      "\n",
      "[223 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving code to new file\n",
    "# notbook-api-seed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap with ETL flow here: \n",
    "- Extract: upload files, crop images, ocr for text; but also need some info from API too\n",
    "- Transform: prepare standardized expenses\n",
    "- Load: send to data base for app to use via API\n",
    "\n",
    "The interesting part about the exercise is having some front-end UI tooling to help with data entry. In some sense there's a \"meta-ETL\" happening here:\n",
    "- Extract: grab as much data as possible\n",
    "- Transform: clean up gathered information\n",
    "- Load: generate final report for expenses for taxes\n",
    "\n",
    "```\n",
    "files...(E->T->L)->T->L...report\n",
    "                   ^UI\n",
    "```         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
