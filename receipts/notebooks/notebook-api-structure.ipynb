{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt notebook with API-first structure\n",
    "In the previous attempt (ocr-test.ipynb), I got all the pieces working, but the code was difficult to read, and made moving to a relation database difficult. I'll prototype an \"API-first notebook\" to see if a that would be a better structure to bridge the data/backend/frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installs, imports\n",
    "%pip install -q \\\n",
    "    pandas\n",
    "\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define \"API\" first, where each function can be replaced by a simple API call\n",
    "\n",
    "# \n",
    "# *** IN PSEUDO CODE ***\n",
    "# \n",
    "\n",
    "## \"SERVICES\"\n",
    "## notebook: pure functions\n",
    "## webdev:   eg lambda services, easy to isolate and scale horizontally as needed\n",
    "\n",
    "# upload images\n",
    "def uploadImage(rawFileURL):\n",
    "    # notebook: move files from `upload` folder to `rawFiles` folder\n",
    "    # webdev: write to temporary storage, keep files in case crop doesn't do a good job and needs to be reverted by the user\n",
    "    uploadedFileURL = './store/rawFiles/' + rawFileURL.split('/')[-1]\n",
    "    shutil.move(rawFileURL, uploadedFileURL)\n",
    "    return uploadedFileURL\n",
    "# crop/convert/postprocess images\n",
    "def cropImages(rawFiles):\n",
    "    # write to storage\n",
    "    # write to postProcessedFiles table\n",
    "    return [postProcessedFile1.Id, postProcessedFile2.Id, postProcessedFile3.Id] # could return success/failure, but prefer to return IDs for reference, might want to return objects depending on how it'll be used later\n",
    "# ocr images to create receiptTexts\n",
    "def ocrImage(croppedImage):\n",
    "    # contains data for boundingbox, text\n",
    "    # contains reference to filename\n",
    "    return [receiptText1.Id, receiptText2.Id, receiptText3.Id]\n",
    "# create receipt\n",
    "# def createReceipt(receiptTexts):\n",
    "#     # contains refrence to receiptTexts, filenames via receiptTexts # note one receipt could have multiple images, and texts across these images\n",
    "#     # write to receipts table\n",
    "#     return receipt1.Id\n",
    "\n",
    "## \"DATA\"\n",
    "## notebook: pandas dataframes-->ORM-->to API external call directly\n",
    "## webdev:   eg API create endpoints\n",
    "\n",
    "# define/import referenceItems\n",
    "def writeReferenceItem(name, quantity, unitOfMeasure, price, pricePerWeight, referenceUrl):\n",
    "    # contains data for name, quantity, unitOfMeasure, price, pricePerWeight, referenceUrl\n",
    "    # write to referenceItems table\n",
    "    return referenceItem1.Id\n",
    "\n",
    "# define/import search querries tied to referenceItems\n",
    "def writeEligibleProduct(productName=\"Schar Gluten Free Hot Dog Buns\", referenceItem=\"Hot dog buns\"):\n",
    "    # contains data for productName, referenceItem\n",
    "    # write to eligibleProducts table\n",
    "    return searchQuerry1.Id\n",
    "\n",
    "def writeEligibleExpense(description, amount, date, receiptTextId,referenceItemId):\n",
    "    # contains data for description, amount, date, receiptTextId,referenceItem\n",
    "    # write to eligibleExpenses table\n",
    "    return eligibleExpense1.Id\n",
    "\n",
    "## \"PROCESSING\"\n",
    "## notebook: impure functions on pandas dataframes\n",
    "## webdev:   backend controllers / helpers, harder to isolate\n",
    "\n",
    "# parse receiptTexts against possible product names to find and create eligible eligibleExpenses (description, amount, date, searchQuerry, referenceItem)\n",
    "def checkForEligibleProductName(receiptTextId):\n",
    "    eligibleProduct = None\n",
    "    def checkProductName(receiptTextId):\n",
    "        # check if text in receiptText is in eligibleProducts\n",
    "        return True\n",
    "    def lookForProductPrice(receiptTextId):\n",
    "        # bunch of magic here\n",
    "        return price\n",
    "    if checkProductName(receiptTextId):\n",
    "        price = lookForProductPrice(receiptTextId)\n",
    "        eligibleProduct = ( checkProductName(receiptTextId), lookForProductPrice(receiptTextId) )\n",
    "    return eligibleProduct\n",
    "\n",
    "def parseTextForEligibleExpenses(receiptTextId): \n",
    "    if checkForEligibleProductName(receiptTextId):\n",
    "        writeEligibleExpense(priceEach, quantity, receiptTextId, referenceItemId)\n",
    "    return # success/failure? not sure yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding\n",
    "\n",
    "There's still a step for seeding the referenceItems table. In a notebook, that's loading the data in pandas dataframe; but a product that's data that is already in the app for all users to use, and this is problematic in this data structure because there should be referenceItems that are public to everyone and private to the user. For simplicity, I will assume all referenceItems are shared between all users for the purpose of the prototype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEEDING\n",
    "# create referenceItems\n",
    "referenceitems = pd.read_csv('referenceitems.csv')\n",
    "for referenceitem in referenceitems:\n",
    "    createReferenceItem(\n",
    "        referenceitem['name'], \n",
    "        referenceitem['quantity'], \n",
    "        referenceitem['unitOfMeasure'], \n",
    "        referenceitem['price'], \n",
    "        referenceitem['pricePerWeight'], \n",
    "        referenceitem['referenceUrl']\n",
    "        # TODO: likely should have one to many relationship with searchQuerries by ID (i.e. different text strings it appears as on receipts)\n",
    "    )\n",
    "\n",
    "# create searchQuerries, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function stuff\n",
    "\n",
    "This is where the two worlds meet, but are different in how they would be approached. \n",
    "\n",
    "A notebooks flow would look like: \n",
    "- for image in images_in_folder process and ocr all images; analyze data\n",
    "- for each string for each receipt: look for eligible expenses; analyze data\n",
    "- for each eligible expense join on reference item info; analyze data\n",
    "- export a single table with all expenses and information\n",
    "\n",
    "(!) this is the main difference in the two approaches / ways of thinking\n",
    "It's a bit easier to think in batches in a notebook (eg ocr all images, parse all texts, etc.) but that's not good for a webdev flow (mostly because it doesn't work well with relational data).\n",
    "\n",
    "consider the user journey:\n",
    "- select images to upload, wait\n",
    "- get a list of receipts to manually review, edit expenses, save (and ideally mark as reviewed, but not for minimum viable prototype)\n",
    "\n",
    "so a webdev flow would look like:\n",
    "1. select one or multiple images, upload\n",
    "2. background chron job to process ocr on each image, look for eligible expenses, match to reference items\n",
    "3. line item level update mutations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one approach is waterfall\n",
    "\n",
    "- main(): \n",
    "    - \"upload\" images (for file in files_in_folder)\n",
    "        - cropImage\n",
    "            - ocrImage (returns receiptTexts)\n",
    "                - parse text for eligible expenses\n",
    "\n",
    "another approach is to do it async as chron jobs, many advantages (scalability, error handling)\n",
    "- \"upload\" images (go through folder)\n",
    "- cropImages that were uploaded but not cropped\n",
    "- ocrImage that were cropped but not OCRed\n",
    "- parse receiptTexts for eligible expenses line items\n",
    "- chron() to run every so often\n",
    "\n",
    "... on the front-end the user will then pick up this data and edit individual Expense line items\n",
    "\n",
    "The async chron job approach above might be a good match for notebooks in the sense that parse each step in bulk before to look at the results before building the block of code. Arguably code can be written sequentially right away, or refactored later, but my objective is to make it easy on both sides to be able to easily build support tooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attempt: waterfall (see `main()` in [ocr-test.ipynb])\n",
    "# I originally tried writing my notebook in the first approach, where I would create a function in a block above the main function block, and have the main function run the `for` loop with reference to all the individual pieces, but I found it hard to read and edit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second attempt: async\n",
    "\n",
    "import time # mostly for testing\n",
    "import threading\n",
    "\n",
    "stop_flag = False\n",
    "\n",
    "def chron():\n",
    "    while not stop_flag:\n",
    "        # check for raw files to crop\n",
    "        # ... probably by looking up at a table of rawFiles that were \"uploaded\"\n",
    "        #     ^ this is the key difference, there isn't an entire dataframe being passed to the next step\n",
    "        # ... tempNewRawFiles = ...\n",
    "        # cropImages(tempNewRawFiles)\n",
    "\n",
    "        # check for croppedImages to ocr\n",
    "        # ... tempNewCroppedImages = ...\n",
    "        # ocrImage(tempNewCroppedImages)\n",
    "\n",
    "        # check for receiptTexts\n",
    "        # ... tempNewReceiptTexts = ...\n",
    "        # parseTextForEligibleExpenses(tempNewReceiptTexts)\n",
    "\n",
    "        # 🏁 now there should be Expenses created, ready for user to manually review and modify\n",
    "\n",
    "        time.sleep(0.500) # slow things down for testing\n",
    "        pass\n",
    "\n",
    "def stop_chron():\n",
    "    global stop_flag\n",
    "    stop_flag = True\n",
    "    watch_thread.join()  # Wait for the thread to finish\n",
    "    pass\n",
    "\n",
    "watch_thread = threading.Thread(target=chron)\n",
    "watch_thread.start()\n",
    "# elsewhere I can use `stop_chron()` to stop the thread\n",
    "\n",
    "# kick things off, note this parrallels nicely what the action the use would take\n",
    "def main():\n",
    "    # \"upload\" images\n",
    "    # rawFiles = ... # from os folder list etc\n",
    "    # for rawFile in rawFiles:\n",
    "    #     uploadImage(rawFile)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a webdev perspective I would have a docker instance running the above code\n",
    "# here I turn everything off for the purpose of using the notebook\n",
    "stop_chron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
